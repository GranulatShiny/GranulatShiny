---
output:
  html_document:
    toc: true
    toc_float: true
  pdf_document:
    toc: true
  word_document:
    toc: true
---

```{=html}
<style>
body {
text-align: justify}
</style>
```
# <i class="fa-solid fa-book"></i> User guide of GranulatShiny

Authors : Aurel Hebert--Burggraeve, Laure Simplet, Laurent Dubroca, Camille Vogel <br>

```{r, echo=FALSE,  out.width = "50%", fig.align = "left"}
knitr::include_graphics("inst/app/www/favicon.png")
```

## <i class="fa-solid fa-dungeon"></i> Homepage

```{r, echo=FALSE, warning=FALSE, results = FALSE, message=FALSE}
library(shiny)
library(DT)
library(ggplot2)
library(cowplot)
library(dplyr)
library(leaflet)
library(RColorBrewer)
library(lme4)
library(DHARMa)
library(shinydashboard)
library(ggeffects)
library(vegan)
library(pairwiseAdonis) #library(devtools) ; install_github("pmartinezarbizu/pairwiseAdonis/pairwiseAdonis")
```

<br> GranulatShiny is an application that facilitates the statistical processing of data collected as part of the initial studies, pre-construction baseline studies and environmental monitoring dedicated to fishery resources and ichthyofauna relating to the appraisal of applications for authorisation to extract marine aggregates. The application automates some of the data formatting and the calculation of standard biodiversity indicators, and provides decision keys for the more advanced processing stages.<br> Based on the calculated indicators and the user's choices, the application produces figures and tables in formats corresponding to the recommendations of the french reference documents: the "Halieutic Protocol" and the methodological guide for the development of Orientation Documents for Sustainable Management of Marine Aggregates (DOGGM). The application provides an interactive graphical interface based on the R language, relieving the user of the need for mastery to focus on parameters of interest for diagnosing the potential effects of marine aggregate extraction on fish resources. GranulatShiny consists of 3 statistical approaches (exploratory, descriptive, and inferential) that can be used primarily to quantify the influence of marine aggregate extraction on fish communities. Each approach covers an expected aspect of the "Halieutic Protocol". The exploratory approach presents and analyzes data at the scale of the entire community. The descriptive approach presents and analyzes data at the scale of a species. And resorting to inferential analysis is necessary to evaluate the temporal and spatial variability of different indicators of fish resources before and during exploitation.<br> The current version of the application does not have a regulatory purpose but serves as an aid in the production of monitoring reports on fish communities. It does not replace the work already provided by consulting firms but complements what is already done by allowing the assembly of a statistical model to test the effects of certain parameters on fish communities.

This guide has been written to enable a GranulatShiny user to familiarize themselves with the application interface and understand the methodology developed behind each result provided by the application.<br>
Before starting, note that there are different buttons in the application.

The buttons with a boat icon allow you to **switch from one tab to another**.

```{r, echo=FALSE, warning=FALSE}
actionButton("start", "start", icon = icon("ship"))
```

Those bearing a small green dragon indicate a **mandatory stopover**.

```{r, echo=FALSE, warning=FALSE}
actionButton("go", "Mettre en forme",icon = icon("dragon", style='color: #22A433'))
```

The buttons with an arrow allow you to **download results** from the application. The formats used are (csv, png, txt, rds).

```{r, echo=FALSE, warning=FALSE}
actionButton("tel", "Telecharger la table",icon = icon("download"))
```

Finally, those with a circle containing an i are **helps** that can be displayed to help you better understand a graph or other objects proposed by the application.

```{r, echo=FALSE, warning=FALSE}
actionButton("info", "",icon = icon("circle-info"))
```

When launching the application, the homepage opens automatically. On this page, various reference documents and information are listed with their associated URLs hyperlinked. A reminder of the context regarding marine aggregate extraction is located on the right side of the page. <br> To move to the next tab, press the "start" button on the homepage within the application.

## <i class="fa-solid fa-wand-magic-sparkles"></i> Data formatting

### Â Information to enter: loading datasets to analyze.

The first step in using the application is to import the data collected. The data must comply with a standard defined by Ifremer, the outline of which can be found here:<br>
https://raw.githack.com/GranulatShiny/GranulatShiny/main/Description_Format_Generique_GranulatShiny.html
<br> 

There are three possible scenarios: <br>

**- Case 1: You are new to the tool and have no formatted data with which to test its functionality. In this case, you will use the dataset supplied with the tool**<br> If this is an exercise in discovering the tool, a fictitious dataset is made available to the user along with the tool. The data is directly integrated into the application and can be loaded by selecting the "Non" answer under the heading: "Do you have your own data? This dataset is intended for educational purposes. It does not correspond to any real case and cannot therefore appear in documents with administrative value (i.e. monitoring reports, initial environmental status, reference status before works, etc.).

For the purposes of familiarisation with the tool, the dataset made available with the tool corresponds to a fictitious concession located in the Bay of Biscay. For this fictitious dataset, we consider a concession in operation from 2000 to 2030, for which monitoring of the fisheries compartment has been set up every 5 years with 2 years of initial status. The fictitious sampling plan provides for the sampling of 10 stations within the concession and 10 stations outside the concession. This choice does not correspond to a sampling plan that would have been developed with knowledge of the environmental conditions of the site (i.e. sedimentary facies, benthic habitats) and therefore does not correspond to the recommendations of the "Halieutic Protocol". This fictitious sampling plan uses a beam trawl with a horizontal opening of 4.4 m and a tow length of 1000 metres. <br> To avoid any confusion, the 4 species present in this dataset are also fictitious. Each species has a population dynamic associated with a specific probability distribution law. Knowing this, it is possible to control the results from inferential statistics and the effects of the environment on the chosen species. The first species, Cephalaspis.tenuicornis, was not affected by the extraction and its spatio-temporal dynamics were stable throughout the monitoring period (i.e. there was no effect of time, space or environmental conditions on the observed abundances). Thus no potential effect of the variables on the abundance of this species will be detected. The Dimichtys.terreli species is impacted by extraction, but its spatio-temporal dynamics are stable throughout the monitoring period. There is therefore a significant effect of extraction, which is reflected in the difference between the values obtained by sampling in or outside the exploitation zone. Leedsischthys.problematicus is affected by extraction, but differently depending on the season. The seasons do not influence the population of this species in normal times, i.e. in the absence of aggregate extraction, but the interaction between the effect of extraction and the seasonal effect modifies the abundance of this species. Finally, Latimeria.chalumnae is not affected by the effect of extraction, nor by spatio-temporal effects such as season and/or environmental conditions, but the abundance of the species is naturally highly variable. These examples illustrate different responses to the environment in order to better understand what is sought during inferential analysis.<br>

**- Case 2: you have some initial experience of the tool and are starting to analyse your own dataset in the recommended format** <br> If you are analysing real data in the appropriate format, you need to select and load the following files into the GranulatShiny graphical interface: "TuttiCatch.csv" and "TuttiOperation.csv", which contain most of the information relating to the progress and results of the monitoring carried out. Only the csv format is supported. The "TuttiCatch.csv" file corresponds to the catch data from the sampling of fish populations and the "TuttiOperation.csv" file corresponds to all the information derived from the implementation of the protocol for each sampling station (i.e. date and name of the survey, fishing gear, the characteristics of which will be specified in the reports associated with the results, geographical coordinates of the spinning and turning points and associated times, total duration of the trawl haul, turning and spinning depth).<br> **WARNING**. The expected data format must be respected, otherwise the processing routines cannot run correctly and a warning message will appear on the interface. In this case, it is recommended that you review your file format with the expected file format. Furthermore, in the case of sub-sampling or information provided at individual level, it is important to report the information at tow scale so that each combination of species and tow corresponds to a single line in the data table. Otherwise a message will be returned indicating the existence of duplicates preventing the catch data from being processed. <br>

Once the files have been loaded, you will have access to a number of new functions. A map centred on the concession will appear, displaying the sampling stations. You will also be able to interact with the 'Impact stations' and 'Reference stations' fields. You will also be able to import 'ShapeFiles' to display the contours of the marine aggregate extraction concession. <br> Under the heading "Impact stations", you can check and modify the operating period (the period during which extraction work takes place). You must also enter in the corresponding space the stations that are impacted by the extraction. The colour of the various samples will then change to red for the stations affected (see figure below).<br> In accordance with paragraph 8.3.2 of the "Halieutic Protocol", the application is developed for the most common case of trawl sampling. The horizontal opening length of the trawl must be entered to calculate the sampled areas in order to work in density. At this stage of development, the application does not take into account other fishing gears.<br>Finally, in the exceptional case where a station entered in the "TuttiOperation.csv" file needs to be removed after the fact, this can be done under the "Reference stations" heading.<br>

```{r, echo=FALSE, warning=FALSE}

tutti_operation <- readRDS("data/operation.rds")
map_station <- tutti_operation %>% dplyr::filter(Annee == 1999)
polygon <- readRDS("data/polygon.rds")

#leaflet
carte <-
  leaflet() %>% addTiles() %>% addPolygons(
    data = polygon,
    opacity = 1,
    dashArray = "5,10",
    label = "Zone de la concession",
    labelOptions = labelOptions(textsize = "15px"),
    weight = 3,
    fillOpacity = 0.1,
    color = "black"
  )

for (i in 1:nrow(map_station)) {
  if (grepl("H", map_station$Code_Station[i])) {
    carte <-
      carte %>% addPolylines(
        lng = c(map_station$LongDeb[i], map_station$LongFin[i]) ,
        lat = c(map_station$LatDeb[i], map_station$LatFin[i]),
        label =  paste(
          map_station$Code_Station[i],
          ": Zone",
          map_station$zones[i],
          "Station impactÃ©e du",
          as.Date(map_station$impact_date_1[i]),
          "au",
          map_station$impact_date_2[i]
        ),
        labelOptions = labelOptions(textsize = "15px"),
        color = brewer.pal(n = 9, name = "Reds"),
        opacity = 0.8
      )
  } else {
    carte <-
      carte %>% addPolylines(
        lng = c(map_station$LongDeb[i], map_station$LongFin[i]) ,
        lat = c(map_station$LatDeb[i], map_station$LatFin[i]),
        label = paste(map_station$Code_Station[i], ": Station de rÃ©fÃ©rence"),
        labelOptions = labelOptions(textsize = "15px"),
        color = brewer.pal(n = 9, name = "Blues"),
        opacity = 0.8
      )
  }
}
carte <- carte %>% setView(lng = map_station["LongDeb"][1,1],
                             lat = map_station["LatDeb"][1,1], zoom = 11)
carte
```

**- Case 3: You have already used the tool to process your data. You have a summary file of all the parameters used for a previous analysis and you want to start again from this file.** <br> If you have already saved the settings in a file, you can import them after the "TuttiCatch.csv" and "TuttiOperation.csv" files, so that the station fields are filled in automatically. <br>

### Production of indicator tables

When you have completed the data loading stage, you can press the button with the green dragon. This will launch the internal calculation of the various indicators and covariates required to analyse the data. If you do not press this button, nothing will happen and you will not be able to continue with the analysis. <br>

**Nota Bene :** If you have more than one concession to analyse, you can return to this tab, change the files by loading those corresponding to this other concession ("TuttiCatch.csv", "TuttiOperation.csv", "ShapeFiles"), then press the green dragon again to restart production of the indicator tables.

### General table

In the "Tables" tab, there is a data table on the right and an interactive section on the left. The table displayed is formed from the data entered in the 'Data formatting' tab. The table formatting functions will calculate abundance, biomass and various diversity indicators for each station and for each survey. The "treatment" variable, which indicates the state of each station, can take two values: "no impact" or "impact". It indicates whether the station is within the perimeter of the concession and therefore considered to be impacted by the aggregate extraction work (i.e. "impact" mode) or whether the station is outside the perimeter of the concession (i.e. "no impact" mode). In the case of an initial state, where there has been no extraction on the site of the concession studied, the stations located inside the concession are assigned the "no impact" state until the start date of exploitation. This allows them to be considered as reflecting the state of the environment before any impact from extraction, for the purposes of the statistical analysis carried out afterwards.

```{r, echo=FALSE, warning=FALSE}

dataset <- readRDS("data/complete_dataset.rds")
dataset$saison <- factor(dataset$saison, levels = c("Winter","Spring", "Summer", "Autumn"))
dataset <- dataset[, 2:length(dataset)]

# Define the callback function to add custom HTML and CSS
callback <- JS(
  '$(document).ready(function() {',
  '  var container = $(".dataTables_wrapper");',
  '  container.css("overflow-x", "auto");',  # Enable horizontal scrolling
  '});'
)

# Create the datatable with the callback function
datatable(dataset, callback = callback)

```

During the formatting processes, the season mode is calculated on the basis of the sampling start dates. The administrative framework is chosen by default to determine the seasons. However, it is possible to change this column in the full table. Particular attention is paid to the notion of season, as this is an integral part of the assessment of the temporal variability of fish communities (paragraph 8.1.4 of the Halieutic Protocol). According to the "Halieutic Protocol", the effects of seasonal variability on fish assemblages (groups of species) are highly dependent on latitude. In northern waters (North Sea, English Channel, northern Bay of Biscay), it is common to observe only two types of fish assemblages per year, a winter assemblage for about eight months of the year and a summer assemblage for about four months. In the warmer waters of the south (south of the Bay of Biscay, Mediterranean), seasonal assemblages are potentially more numerous, with more marked spring and autumn assemblages. Nevertheless, it is the conduct of the initial survey that will make it possible to determine the seasonal variability locally and to decide on the seasonal periodicity of the monitoring surveys. By modifying the "season" column in the table, it is possible to adjust to local conditions and sampling difficulties.

You can change the general display of the data table using the arrow under the "which table to display" message, and you can download the table displayed using the "Download table" button. The "Download entered information" button is used to save the list of impact stations, the exploitation dates and the trawl opening width used in the "Import data" tab in a csv file. At the end of this tab, you can decide whether to carry out the "exploratory statistics" section, which looks at the community as a whole, or to go straight to the "descriptive statistics" section, which focuses on a specific variable.

## <i class="fa-solid fa-eye"></i> Exploratory statistics

### Representation of indicators

This section looks at the biodiversity and abundance indicators obtained at the community level, inside and outside the concession, for each data collection survey carried out. The indicators presented are those referred to in the "Halieutic Protocol", article 8.4.1.

#### Â Â Definition of indicators

Biodiversity encompasses the variety of life at all levels of organisation, classified according to evolutionary (phylogenetic) and ecological (functional) criteria. At the level of biological populations, genetic variation between individual organisms and between lineages contributes to biodiversity as a signature of evolutionary and ecological history and a basis for future adaptive evolution. It is at the species level that the term biodiversity is most often applied by ecologists and conservation biologists. Species richness refers to the total number of species present in a given ecosystem. It is a simple measure that only takes into account the number of species without considering their relative abundance. For example, if a tropical forest contains 100 different tree species, its species richness would be 100.

A diversity index is a mathematical expression that combines species richness and evenness to measure diversity. The main objective of a diversity index is to obtain a quantitative estimate of biological variability that can be used to compare biological entities in space or time. This index takes into account two different aspects that contribute to the concept of diversity in a community: species richness and homogeneity.

The Shannon-Weaver diversity index is a widely used index for comparing diversity between different habitats. It assumes that individuals are randomly sampled from a large independent population and that all species are represented in the sample. This index measures both species richness and the equity (or uniformity) of species distribution in an ecosystem. It takes into account both the number of species present and their relative abundance. More specifically, the Shannon-Weiner index is calculated using the following formula:

$$ Hâ²= -\sum_{i}^S (p_{i}*ln(p_{i})) $$

oÃ¹ :<br> **S** is the total number of species, <br> **pi** is the proportion of the i-th species among all the species present, <br> **ln** represents the neperian logarithm

The value of the Shannon-Weaver diversity index is generally between 1.5 and 3.5 and rarely exceeds 4.5. A higher Shannon-Weiner index indicates a greater diversity of species and a more uniform distribution between these species.

Unlike the Shannon-Weiner index, the Simpson index focuses primarily on the dominance of the most abundant species in an ecosystem. It is calculated using the following formula:

$$ D= \sum_{i}^S (p_{i}*(p_{i}-1)) $$

where the terms are the same as in the Shannon-Weiner index. A higher Simpson index indicates lower biodiversity, because it places greater emphasis on the probability that a species chosen at random is the same as the one chosen previously.

#### Â Â Representation in the application

Firstly, the table (below) shows the mean values for abundance, biomass, species richness, Shannon and Simpson indicators inside the concession, outside the concession and overall for each survey. They are calculated from the values obtained at each sampling station. For easier reading, the standard deviations are not displayed in the table in the application but are available in the cvs file that can be downloaded via the "Download table" button.

```{r, echo=FALSE, warning=FALSE}
source("R/fct_diversity_table.R")

ID_campagne <- c()
for (i in (1:max(dataset['campagne']))){
  ID_campagne <- c(ID_campagne, paste("C",i,sep=""))
}


data_indic <- cbind(
    "ID_campagne" = ID_campagne,
  diversity_table(dataset, "Abun"),
  diversity_table(dataset, "Biom" ),
  diversity_table(dataset, "Richness" ),
  diversity_table(dataset, "Shannon" ),
  diversity_table(dataset, "Simpson" )
)

name_indic <- names(data_indic)
position <- grep("mean", name_indic)
show_data_indic <- data_indic[,position]
show_data_indic <- data.frame("ID_campagne" = data_indic[,1], show_data_indic)

datatable(show_data_indic, callback = callback)
```

The graphs below show the mean values (dots) and the 5 and 95 percentiles (high and low bars) obtained for the same indicators as those in the table, depending on the survey selected and the sector sampled (paragraph 8.4.1 of the Halieutic Protocol). They provide a quick overview of the differences in values obtained between the concession area and the reference area for the most common biodiversity indicators.

```{r, echo=FALSE, warning=FALSE, fig.height = 9, fig.width = 16}
source("R/fct_lineplot_creation.R")
campagne <- "C1"
abun_plot <- lineplot_creation(data_indic, "Abun", campagne)
biom_plot <- lineplot_creation(data_indic, "Biom", campagne)
richness_plot <- lineplot_creation(data_indic, "Richness", campagne)
shannon_plot <- lineplot_creation(data_indic, "Shannon", campagne)
simpson_plot <- lineplot_creation(data_indic, "Simpson", campagne)
frise <- plot_grid(abun_plot, biom_plot, richness_plot,
                       shannon_plot, simpson_plot)
frise
```

<br> The advantage of these approaches is that the fish community can be compared on several scales. Initially, the comparison focuses on the inside or outside of the concession. But if the surveys are looked at one after the other, it may be possible to distinguish changes over time. There is both a spatial and a temporal aspect.

### Representation of the structure

This table represents the proportion of each species present for each sampling survey (paragraph 8.4.1 of the Halieutic Protocol). The table makes it possible to monitor changes in the proportions of species over time and provides a perspective on trends in assemblages. Variations in the proportions of different species from one year to the next can indicate significant ecological changes, such as fluctuations in biodiversity, changes in habitats or environmental pressures. This table can also be used to identify which species are dominant in a given ecosystem and which are in decline.

```{r, echo=FALSE, warning=FALSE}
source("R/fct_structure_table.R")

catch <- readRDS("data/catch.rds")
species <- unique(catch['Nom_Scientifique'])[,1]

data_brut <- data.frame(ID_campagne)
for (sp in species){
  data_brut <- cbind(data_brut, structure_table(dataset, sp))
}
position <- grep("tot_value", names(data_brut))
data_brut <- data_brut[,c(position)] 
tot <- rowSums(data_brut)
data_brut <- cbind(ID_campagne, data_brut, tot)
names(data_brut) <- c("ID_campagne",species,"Total")



for (i in 1:nrow(data_brut)){
   for (sp in 1:length(species)){
     data_brut[i,sp+1] <- round(data_brut[i,sp+1]/data_brut$Total[i], digits = 2)
   }
}
data_percent <- t(data_brut[,2:(length(species)+1)])
#redefine row and column names
colnames(data_percent) <- ID_campagne
data_percent <- as.data.frame(cbind(species,data_percent))
names(data_percent)[1] <- c("species")
datatable(data_percent, callback = callback)

```

The figure below is made up of three graphs representing the abundance of species in a given survey in descending order (left), the relative contribution of each species to the total abundance (top right) and the species accumulation curve (bottom right). It provides information to meet the expectations of paragraph 8.4.1 of the " Halieutic Protocol ".

```{r, echo=FALSE, warning=FALSE, fig.height = 9, fig.width = 16}
#### prepare data ####
      data_t <- as.data.frame(t(data_brut[,2:(length(species)+1)]))
      #rownames(data_t) <- species
      colnames(data_t) <- ID_campagne
      data_t <- cbind(species,data_t)
      names(data_t) <- c("species", names(data_t)[2:length(names(data_t))])

      # order the data to have the most present species
      data <- as.data.frame(data_t[c("species","C1")])
      names(data) <- c("species", "Abundance")
      #data <- data %>% dplyr::arrange(desc(Abundance))
      data <- data[order(data$Abundance, decreasing = TRUE), ]
      data$species <- factor(data$species,data$species)

      #### barplot ####
      # Default bar plot
      barplot <- ggplot(data, aes(x=species, y=Abundance, fill=species)) +
        geom_bar(stat="identity", color="black",
                 position=position_dodge())+
      # Finished bar plot
      labs(title=paste("Abundance per species for the survey ",
                         "C1", sep=""),
           fill = "")+
        theme_classic() +
        # Reduce the size of the plot
        theme(
          legend.position = "bottom",
          legend.box = "horizontal",
          legend.margin = margin(t = 0, unit = "cm"), # Adjust margin if needed
          legend.text=element_text(size=8))+ # Adjust legend text size if needed
        scale_fill_viridis_d()
      # change the display of species for too many species
      if (length(data$species) > 5){
        # Set the threshold for displaying species names
        threshold <- 5  # Adjust this value based on your preference

        # Get the species names and filter them based on the threshold
        species_names <- data$species
        filtered_species_names <-
          ifelse(seq_along(species_names) %% threshold == 0, species_names, "")

        # Update x-axis labels with filtered species names
        barplot <- barplot + scale_x_discrete(labels = filtered_species_names)}
      
      #### cumulative plot ####
      # Create a cumulative sum of abundances
      data$cumulative_abundance <- cumsum(data$Abundance)

      # Create the cumulative abundance curve with number of species
      # on the x-axis using ggplot2 and geom_step
      cumul_plot <- ggplot(data, aes(x = seq_along(cumulative_abundance),
                                          y = cumulative_abundance)) +
        geom_step() +
        labs(title = "Cumulative Abundance Curve",
             x = "Number of Species",
             y = "Cumulative Abundance")
      
       #### Species accumulation curves ####
      indice_campagne <- 1
      SAC <- specaccum(dataset %>%
                         dplyr::filter(campagne==indice_campagne) %>%
                         dplyr::select(species), "random")
      table_SAC <- data.frame(site = SAC$sites, richness = SAC$richness,
                              sd = SAC$sd)# table_SAC export the information
      #from the list generate by speccacum
      SAC_plot <- ggplot(table_SAC, aes(site))+
        geom_ribbon(aes(ymin = richness - sd,
                        ymax = richness + sd), fill = "lightblue")+
        geom_line(aes(y=richness), color="blue")+
        #geom_errorbar(aes(ymin=richness-sd, ymax=richness+sd), width=.2,
        #              position=position_dodge(0.5)) +
        labs(title = "Species accumulation curves",
             x = "Number of Sites",
             y = "Number of Species")

      ### prepare the plot to be display ####
      p <- plot_grid(cumul_plot, SAC_plot, ncol = 1, rel_heights = c(5,5))

      plot_grid(barplot,p, ncol = 2, rel_widths = c(5,4))
```

The figure on the left shows the abundance of each species for a survey in a histogram ordered by decreasing abundance. This makes it possible to identify the dominant species within a survey, i.e. those that are most abundant in the sample. These dominant species can play a crucial role in structuring the ecosystem studied, influencing, for example, competition for resources or predation on other species. <br> In addition, by observing changes from one survey to the next, this histogram can help to visualise temporal trends. For example, an increase or decrease in the abundance of a dominant species could indicate changes in environmental conditions or in the interactions between species. It can also be used to detect seasonal and annual variations, such as breeding peaks or seasonal migrations, which can influence the composition of the community. <br> Comparing data between different surveys is also made easier by this histogram. By placing species abundance distributions for different periods side by side, it can help to identify similarities and differences between ecosystems at different times of the year. This comparison can reveal general ecological patterns or specific responses to environmental disturbances. <br> For ease of reading, species accounting for less than 1% of total abundance are not shown on the histogram.

The figure in the top right corner is a curve of cumulative abundance as a function of the number of species in descending order of abundance. Cumulative abundance refers to the cumulative sum of species abundances in a dataset, starting with the most abundant species and successively adding the abundances of the following species in descending order. The cumulative abundance curve allows for the assessment of species diversity and distribution in an ecosystem or biological sample. The straighter the curve, the more diversified the community, whereas a curve that rises quickly then flattens indicates a community where a few species are very abundant while most species are rare. It's important to note that the cumulative abundance curve is of interest when there are many different species. Here, the curve is constructed from the dataset of a fictitious concession with only 4 species. Therefore, it has limited relevance. In practice, you should not have this kind of result with your data.

The figure in the bottom right corner represents the number of species as a function of the number of sampled sites. The algorithm, for a given number of sites, will test all existing combinations in the dataset and retrieve the number of species for each combination. Then it calculates an average per number of sites, and it's this average value that is plotted on the graph.<br>
The shape of the species accumulation curve can provide information about the diversity of the studied ecosystem. If the curve increases rapidly and tends towards a flat asymptote, it suggests that most of the present species have been sampled, providing a good estimate of ecosystem diversity. Conversely, if the curve increases slowly and does not seem to reach a plateau, it indicates that there are still species to be discovered, and sampling should be continued to obtain a more accurate estimate of diversity.<br>
The species accumulation curve is useful for determining the minimum number of samples needed to obtain an adequate representation of species diversity. This can be used during the initial assessment phase to ensure that the proposed sampling plan captures the diversity of the area. Here, constructed from the fictitious dataset, the curve holds little interest.

## <i class="fa-solid fa-scroll"></i> Descriptive statistics

### Data representation

In this section, we focus on a specific indicator (species abundance, total biomass, diversity index, etc.) and compare it to the explanatory variables in our dataset. We are looking for potential effects or correlations upstream of inferential statistics. Therefore, we need to select a variable that we aim to explain based on parameters related to data acquisition and extraction. The statistical analysis will be conducted on this variable.<br>
Initially, the table summarizing the explained variable provides information on the number of zeros and missing values, the total length of the value series, and the fraction of zeros and missing values compared to the total values. Additionally, it also gives the mean, extremes, standard deviation, and quartiles of the series. The most important values for the "inferential statistics" part are those indicating the number of zeros and missing values ("n_missing", "complete_rate"). The proportion of zeros in the data structures the modeling method to be employed. Indeed, a high proportion of zeros can compromise the implementation of a generalized linear mixed model.  <br>

```{r, echo=FALSE, warning=FALSE}
source("R/fct_numeric_summary.R")

dataset$year <- as.factor(dataset$year)
dataset$campagne <- as.factor(dataset$campagne)

data <- dataset$Abun
datatable(numeric_summary(as.numeric(data), "Abun"),
         callback = callback) 

```

Following this table, it's possible to visualize boxplots. The boxplot provides another representation to interpret the relationship between the explained variable and the explanatory variables such as impact, year, campaign, station, and season. In graphical representations of statistical data, the boxplot is a quick way to illustrate the essential profile of a quantitative statistical series. The boxplot summarizes some position indicators of the studied characteristic (median, quartiles, minimum, maximum, or deciles). It is often used to quickly compare two series.
In GranulatShiny, the series of the explained variable (here abundance) in the impacted zone is compared with that of the non-impacted zone. In the figure below, the same variable is represented on a decimal scale (on the left) and on a logarithmic scale (on the right). The logarithmic scale is offered in the application to transform the variable into a pseudo-normal distribution and provide more meaning to the boxplot representation by limiting the influence of extreme values. Each boxplot is constructed as follows: the horizontal line crossing the white square corresponds to the median, the upper and lower edges of the white square correspond to the 75th and 25th quartiles respectively, the ends of the "whiskers" correspond to the 95th and 5th percentiles; finally, the points correspond to extreme values.

```{r, echo=FALSE, warning=FALSE}

b1 <- ggplot(dataset, aes(x = traitement, y = Abun))+
  geom_boxplot()+
  labs(title = paste("Boxplot of ", "Abun"," by impact", sep=""),
                x = "", y = "")

b2 <- ggplot(dataset, aes(x = traitement, y = log(Abun+1)))+
  geom_boxplot()+
  labs(title = paste("Boxplot of ", "log(Abun)"," by impact", sep=""),
                x = "", y = "")
plot_grid(b1,b2)

```

In paragraph 8.4.1 of the "Halieutic Protocol", it is indicated that the data should be described and analysed by size group, maturity or functional group. The GranulatShiny application does not allow data tables to be subdivided into subgroups. This must be done upstream by the user. To analyse a particular functional group, you need to sort your "TuttiCatch" file and save this new table so that you can integrate it into the application. This enables the GranulatShiny statistical method to be applied to the species to be treated separately.

Once you have explored the data, you can move on to the next tab by pressing the "Choose distribution probability" button or by clicking on "Diagnosis of analysis".

### Diagnostic analysis

This tab allows you to select and visualize the probability distribution that best fits the explained variable. The frequency histogram of the variable under study (selected by the user) is represented by gray bars. It depicts the empirical distribution of the observed data. It's constructed by grouping the data of the variable into intervals and counting the number of observations in each interval. This provides a visualization of the distribution of the variable's values.<br>
The density function (in blue in the example) is an estimation of the probability distribution of the variable's data. It's calculated by fitting different statistical distribution models to the observed data. These models may include normal, Poisson, exponential distributions, etc. The density function represents the probability that an observation falls within a particular range of values.<br>
The probability distribution (in green in the example) represents the statistical distribution model that would best match the density function. It's chosen by the user to best overlay with the blue curve. The parameters of each probability distribution are approximated using the mean and standard deviation of the variable.

By examining these three elements together, you can visually assess how well the adjusted probability distribution model fits the observed data. A good match between the three indicates that the model is a precise representation of the data distribution. However, significant discrepancies may indicate inadequacies in the chosen model or particular characteristics of the data that require further analysis. You can change the type of probability distribution to test which one seems to fit best. If the chosen distribution does not fit at all, a warning message appears. In the example, abundance is represented, and the chosen law is a Lognormal distribution. <br>

```{r, echo=FALSE, warning=FALSE}
source("R/fct_probability_distribution.R")

ggplot(data = dataset, aes(x = Abun, y = after_stat(density))) +
        geom_histogram(colour = "black", fill = "grey", bins = 100) +
        geom_density(alpha = .3, fill = "blue") +
        geom_area(aes( y= probability_distribution(dataset$Abun, "Lognormale")),
                  color="darkslategray", fill = "darkseagreen",
                  alpha = 0.4, linewidth = 1)+
        xlab("Abun") +
        theme_minimal()
```

Once you are satisfied with the probability distribution, check the sentence above the "Modeling" button. There are two possibilities. If you have fewer than 30 observations, the sentence says: "You don't have enough values to build a GLM or a GLMM" In this case, you need to change the working variable because there are not enough values to create a relevant model. Conversely, if the volume of data present in the dataset is sufficient (more than 30 observations), you will see: "Once you have chosen a distribution you can move on to building the model" When you are done, press the "Modelling" button.

## <i class="fa-solid fa-fish-fins"></i> Modelling

### Model creation

This section is devoted to creating a model for inferential analysis. The variable being analysed is shown at the top left of the tab and can be modified in the "data representation" tab. The application allows you to perform 3 types of inferential tests: GLMM, GLM and PERMANOVA. This chapter begins with a reminder of the general statistical principles used in the application.<br>

#### Â Â General principles of statistics needed to use the tool

##### Â Â Â Â [Parametric and non-parametric methods]{.underline}

Le domaine des statistiques existe parce qu'il est impossible de collecter des donnÃ©es auprÃ¨s de tous les individus concernÃ©s (population). La seule solution consiste Ã  collecter des donnÃ©es auprÃ¨s d'un sous-ensemble (Ã©chantillon) des individus concernÃ©s, mais le vÃ©ritable objectif est de connaÃ®tre la "vÃ©ritÃ©" sur la population. La population est approchÃ©e en Ã©tudiant des variables descriptives. Chaque variable est un objet statistique, qui peut Ãªtre dÃ©crit par des indicateurs. Les indicateurs statistiques tels que la moyenne, l'Ã©cart-type et les quartiles servent Ã  rÃ©sumer l'information concernant une variable observÃ©e. Lorsque l'on Ã©tudie un Ã©chantillon considÃ©rÃ© comme reprÃ©sentatif, ces indicateurs servent Ã  construire la loi de distribution de la variable Ã©tudiÃ©e. Chaque indicateur correspond Ã  un "paramÃ¨tre" de cette loi de distribution. On considÃ¨re alors que la loi de distribution obtenue pour cette variable Ã  partir de l'Ã©chantillon est applicable Ã  la population. Ãtant donnÃ© que l'on ne peut gÃ©nÃ©ralement pas obtenir de donnÃ©es sur l'ensemble de la population, on ne peut pas connaÃ®tre les valeurs des paramÃ¨tres pour cette population. Il est toutefois possible de calculer des estimations de ces quantitÃ©s pour l'Ã©chantillon. Lorsqu'elles sont calculÃ©es Ã  partir des donnÃ©es de l'Ã©chantillon, ces quantitÃ©s sont appelÃ©es "statistiques". Une statistique estime un paramÃ¨tre. Les procÃ©dures statistiques paramÃ©triques reposent sur des hypothÃ¨ses concernant la forme de la distribution (c'est-Ã -dire une distribution normale) dans la population sous-jacente et sur la forme ou les paramÃ¨tres (c'est-Ã -dire les moyennes et les Ã©carts-types) de la distribution supposÃ©e. Les procÃ©dures statistiques non paramÃ©triques ne reposent sur aucune ou peu d'hypothÃ¨ses concernant la forme ou les paramÃ¨tres de la distribution de la variable dont l'Ã©chantillon a Ã©tÃ© tirÃ©.

##### Â Â Â Â [Linear models]{.underline}

Un modÃ¨le linÃ©aire classique est une mÃ©thode paramÃ©trique qui permet dâÃ©tudier la liaison statistique entre une variable rÃ©ponse **Y** et les variables explicatives **X**. Soit yi la rÃ©ponse de lâindividu **i** et **x<sub>i</sub>** les valeurs prises par les variables explicatives pour cet individu. La relation entre **X** et **Y** peut sâÃ©crire sous la forme : $$Yâ=âÎ± + \sum_jÎ²_jX_j + Îµ $$ oÃ¹ **Îµ** reprÃ©sente les rÃ©sidus du modÃ¨le, la variance de la variable **Y** non expliquÃ©e par les variables explicatives **X**, distribuÃ©e selon une loi normale dâespÃ©rance nulle. Le terme **Î±** correspond Ã  ce quâon appelle lâintercept et **Î²<sub>j</sub>** reprÃ©sente les coefficients estimÃ©s du modÃ¨le des variables explicatives **X<sub>j</sub>**. La variable rÃ©ponse pour un modÃ¨le linÃ©aire doit Ãªtre une variable approximativement normalement distribuÃ©e.

##### Â Â Â Â [Generalized linear models]{.underline}

Les modÃ¨les linÃ©aires trouvent une large application, mais ne peuvent pas gÃ©rer des rÃ©ponses continues clairement discrÃ¨tes ou asymÃ©triques. Par exemple, les variables rÃ©ponses de type âcomptageâ, souvent asymÃ©trique ainsi que les variables binaires comme la prÃ©sence/absence ne suivent pas une loi normale. Les modÃ¨les linÃ©aires ne sont donc pas adaptÃ©s Ã  ce type de variables. Les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s (GLM) permettent l'extension des idÃ©es de modÃ©lisation linÃ©aire Ã  une classe plus large de types de rÃ©ponse, comme celles Ã©noncÃ©es prÃ©cÃ©demment, sous une mÃ©thodologie de modÃ©lisation commune. Une chose importante Ã  comprendre dans les GLM est la relation entre les valeurs de la variable de rÃ©ponse, **Y** (telles que mesurÃ©es dans les donnÃ©es et prÃ©dites par le modÃ¨le dans les valeurs ajustÃ©es) et le prÃ©dicteur linÃ©aire. Le prÃ©dicteur linÃ©aire Ã©merge du modÃ¨le linÃ©aire comme une somme de chaque terme du modÃ¨le. Le prÃ©dicteur linÃ©aire correspond Ã  la variable **Y** seulement lors dâun modÃ¨le linaire classique suivant une loi normale. Dans le cas de modÃ¨le linÃ©aire gÃ©nÃ©ralisÃ©, câest la fonction de lien, **g**, qui relie la valeur **Y** Ã  son prÃ©dicteur linÃ©aire **N**. $$ N =g(Y) $$ La valeur de **N** est obtenue en transformant la valeur de **Y** par la fonction de liaison **g**, et la valeur prÃ©dite de **Y** est obtenue en appliquant la fonction de liaison inverse Ã  **N**. <br> En utilisant diffÃ©rentes lois de distribution et donc diffÃ©rentes fonctions de lien, il est possible dâobserver les consÃ©quences sur les hypothÃ¨ses des rÃ©sidus du modÃ¨le. La fonction de lien la **plus appropriÃ©e** est celle qui produit **les rÃ©sidus les plus conformes**.

##### Â Â Â Â [Generalized linear mixed model]{.underline}

Les modÃ¨les linÃ©aires mixtes gÃ©nÃ©ralisÃ©s (GLMM) sont une extension des GLM. Un GLMM est dit "mixte" parce qu'il comprend au moins un effet "fixe", les variables explicatives et au moins un effet "alÃ©atoire". Les effets alÃ©atoires ne sont pas des termes Ã©valuÃ©s, ils servent uniquement Ã  indiquer au modÃ¨le que les donnÃ©es ne sont pas indÃ©pendantes et reflÃ¨tent une corrÃ©lation entre les unitÃ©s statistiques. D'un point de vue statistique, cela permet d'estimer prÃ©cisÃ©ment la dÃ©viance rÃ©siduelle et donc d'Ã©viter de biaiser l'erreur standard des paramÃ¨tres. Au final, cela se traduit par des p-values plus fiables.

##### Â Â Â Â [Analysis of variance using permutations]{.underline}

La PERMANOVA, ou Analyse de Variance Permutationale MultivariÃ©e, est une mÃ©thode statistique qui permet d'analyser les diffÃ©rences entre plusieurs groupes dÃ©finis par des caractÃ©ristiques qualitatives, comme par exemple les diffÃ©rents traitements dans une Ã©tude expÃ©rimentale.

Contrairement Ã  d'autres mÃ©thodes statistiques qui nÃ©cessitent certaines hypothÃ¨ses sur la distribution des donnÃ©es, la PERMANOVA ne se base pas sur ces suppositions. Elle se focalise plutÃ´t sur une matrice de distance entre les Ã©lÃ©ments Ã©tudiÃ©s. Cette approche lui permet de travailler avec des donnÃ©es de diffÃ©rentes dimensions, qu'elles soient simples ou complexes, et peu importe le nombre de catÃ©gories.

L'objectif de la PERMANOVA est de dÃ©terminer s'il existe des diffÃ©rences significatives dans la variabilitÃ© entre les groupes. Pour ce faire, elle Ã©value la variation entre les groupes (SS inter) par rapport Ã  la variation Ã  l'intÃ©rieur des groupes (SS intra). Une SS inter Ã©levÃ©e suggÃ¨re des diffÃ©rences importantes entre les moyennes des groupes, tandis qu'une SS intra faible indique une similaritÃ© accrue des observations au sein de chaque groupe.

La dÃ©cision de rejeter ou non l'hypothÃ¨se nulle (l'absence de diffÃ©rence entre les groupes) se fait en comparant le rapport entre la variation inter-groupe et la variation intra-groupe Ã  une distribution obtenue par permutation des donnÃ©es. Si ce rapport est significativement Ã©levÃ©, cela indique que les diffÃ©rences observÃ©es entre les groupes sont probablement rÃ©elles.

Cependant, la PERMANOVA prÃ©sente quelques limites. Elle ne permet pas de dÃ©terminer quel groupe spÃ©cifique diffÃ¨re des autres, seulement qu'au moins un groupe est diffÃ©rent. De plus, la prÃ©sence de valeurs nulles peut biaiser l'estimation de la similaritÃ© entre les Ã©lÃ©ments, ce qui est particuliÃ¨rement problÃ©matique en Ã©cologie oÃ¹ un zÃ©ro peut signifier l'absence d'une espÃ¨ce. Cette limitation peut Ãªtre attÃ©nuÃ©e en choisissant un coefficient d'association appropriÃ© dans le calcul de la matrice de distance.

#### Â Â Writing the model with GranulatShiny

Selon la mÃ©thode de modÃ©lisation que vous choisissez, la formulation du modÃ¨le diffÃ¨re. Si l'on prend l'exemple de l'abondance comme variable rÃ©ponse, le GLMM prendra en compte deux variables explicatives fixes, le traitement et la saison, et leur interaction ainsi que deux variables explicatives alÃ©atoires, la campagne et la station : $$GLMM â Abun \sim traitement * saison + (1\|campagne) + (1\|station)$$ Le GLM prendra en compte uniquement les variables explicatives fixes prises en compte dans le GLMM et leur interaction : $$GLM â Abun \sim traitement * saison$$ La PERMANOVA prendra en compte les mÃªmes variables explicatives que le GLM : $$PERMANOVA â Abun \sim traitement * saison$$

Les modÃ¨les sont centrÃ©s sur la variable traitement car le suivi des concessions d'extraction de granulats marins est basÃ© sur la mÃ©thode BACI (Before After Control Impact). Par dÃ©finition, la mÃ©thode BACI compare des sites tÃ©moins (c'est-Ã -dire non impactÃ©s) et des sites impactÃ©s et teste les diffÃ©rences entre l'avant et l'aprÃ¨s. Il s'agit d'une mÃ©thode couramment utilisÃ©e dans la surveillance de l'environnement ocÃ©anique et une mÃ©thode BACI bien conÃ§ue reste l'une des meilleures approches pour les programmes de surveillance des effets sur l'environnement. Malheureusement, cette mÃ©thode prÃ©sente plusieurs limites qui compromettent sa capacitÃ© Ã  dÃ©tecter des effets notamment parce que lâocÃ©an est spatialement et temporellement dynamique, et que trouver deux emplacements statistiquement identiques lâun Ã  lâautre tout en Ã©tant suffisamment Ã©loignÃ©s gÃ©ographiquement pour Ãªtre statistiquement indÃ©pendants constitue un vÃ©ritable dÃ©fi. <br>

Pour un GLMM et un GLM, vous devrez choisir une distribution de probabilitÃ©. Par dÃ©faut, il propose la derniÃ¨re distribution de probabilitÃ© que vous avez vÃ©rifiÃ©e dans la partie prÃ©cÃ©dente. Attention la mÃ©thode utilisÃ©e pour la modÃ©lisation est itÃ©rative, il se peut donc que la distribution qui semblait la plus adÃ©quate dans la partie prÃ©cÃ©dente n'est pas forcÃ©ment celle qui permettra de mieux faire converger le modÃ¨le. NÃ©anmoins l'onglet "diagnostic d'analyse" devrait avoir permis de sÃ©lectionner un nombre de distribution possible pour ne pas avoir Ã  toutes les tester ici.<br>

Vous pouvez Ã©galement conserver ou non l'interaction entre les covariables traitement et saison. Attention si l'interaction n'apporte rien au modÃ¨le celle-ci est retirÃ©e automatiquement. Vous pouvez Ã©galement ajouter d'autres covariables dans votre modÃ¨le. Elles seront ajoutÃ©es sans interaction avec les autres. Lorsque vous Ãªtes prÃªt, vous pouvez cliquer sur "dÃ©marrer la modÃ©lisation".<br>

##### Â Â Â Â [Generalized linear mixed model]{.underline}

C'est la mÃ©thode Ã  prioriser. Dans le paragraphe 8.4.2 du "Protocole halieutique", il est dit que pour Ã©valuer la variabilitÃ© temporelle et spatiale des diffÃ©rents indicateurs des ressources halieutiques avant extraction, il faut utiliser des modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s Ã  effets mixtes (GLMMs) avec les variables temporelle et spatiale dÃ©finies comme effets alÃ©atoires croisÃ©s plus un effet saisonnier fixe. La premiÃ¨re sortie est une reproduction de la sortie du logiciel r pour la ligne de commande correspondante. Vous pouvez choisir d'afficher le tableau d'analyse de la dÃ©viance, qui synthÃ©tise les rÃ©sultats de la modÃ©lisation en ne fournissant qu'un rÃ©sumer de l'Ã©valuation de l'importance des effets fixes dans le modÃ¨le et de comprendre leur impact sur la variable rÃ©ponse. Vous pouvez Ã©galement afficher le rÃ©sumÃ© exhaustif de ces rÃ©sultats. Vous pouvez choisir d'afficher les rÃ©sultats du modÃ¨le avant optimisation via le choix initial ou alors le modÃ¨le optimisÃ© via le choix final en bas Ã  gauche.<br>

Reproduction de la sortie R du modÃ¨le GLMM sur l'abondance.

```{r, echo=FALSE, warning=FALSE}

GLMM <- glmer(log(Abun) ~ traitement * saison + (1|campagne) + (1|station), data = dataset, family = gaussian(link = identity))
GLMM_summary <- summary(GLMM)
GLMM_summary
```

La partie haute de la fenÃªtre de rÃ©sultat rappelle le modÃ¨le qui a Ã©tÃ© utilisÃ© pour calculer les effets.

```{r, echo=FALSE, warning=FALSE}
list(GLMM_summary$methTitle,
c(GLMM_summary$family,GLMM_summary$link),
GLMM_summary$call)
```

Il y a le type de modÃ¨le avec la mÃ©thode de calcul utilisÃ©e. AprÃ¨s, il y a la loi de probabilitÃ© et sa fonction de lien. Enfin, la commande complete est affichÃ©e ce qui permet de vÃ©rifier que la bonne commande a Ã©tÃ© effectuÃ©e.

Ensuite, il est possible de lire des scores liÃ©s Ã  la vraisemblance du modÃ¨le par rapport aux donnÃ©es et aux paramÃ¨tres sÃ©lectionnÃ©s pour la construction du modÃ¨le.

Etant donnÃ© un Ã©chantillon observÃ© **(x<sub>1</sub>,...,x<sub>n</sub>)** et une loi de probabilitÃ© **P<sub>Î¸</sub>**, la vraisemblance quantifie la probabilitÃ© que les observations proviennent effectivement d'un Ã©chantillon (thÃ©orique) de la loi **P<sub>Î¸</sub>**. On appelle vraisemblance associÃ© Ã  la loi de probabilitÃ© **P<sub>Î¸</sub>**, la fonction **L** tel que : $$\displaystyle L(x_1,\ldots,x_n,\theta) = \prod_{i=1}^n P_\theta(x_i)\; $$

```{r, echo=FALSE, warning=FALSE}
GLMM_summary$AICtab
```

Ces indicateurs sont souvent fournis pour Ã©valuer la qualitÃ© de l'ajustement du modÃ¨le et aider Ã  la sÃ©lection du meilleur modÃ¨le parmi plusieurs candidats. <br>
**AIC (CritÃ¨re d'information d'Akaike) :**
L'AIC est un critÃ¨re de sÃ©lection de modÃ¨le qui prend en compte Ã  la fois la qualitÃ© de l'ajustement du modÃ¨le et sa complexitÃ©. Il favorise les modÃ¨les qui s'ajustent bien aux donnÃ©es tout en Ã©tant simples. Un modÃ¨le avec un AIC plus bas est considÃ©rÃ© comme prÃ©fÃ©rable. Cependant, l'AIC ne fournit pas d'indication sur l'ajustement absolu du modÃ¨le, mais seulement sur son ajustement relatif par rapport aux autres modÃ¨les candidats.<br>
**BIC (CritÃ¨re d'information bayÃ©sien) :**
Le BIC est un autre critÃ¨re de sÃ©lection de modÃ¨le qui, comme l'AIC, prend en compte Ã  la fois l'ajustement et la complexitÃ© du modÃ¨le. Cependant, le BIC pÃ©nalise plus sÃ©vÃ¨rement la complexitÃ© du modÃ¨le que l'AIC. Un modÃ¨le avec un BIC plus bas est considÃ©rÃ© comme prÃ©fÃ©rable. Contrairement Ã  l'AIC, le BIC favorise la parcimonie, ce qui signifie qu'il prÃ©fÃ¨re les modÃ¨les plus simples.<br>
**logLik (Log-vraisemblance) :**
La log-vraisemblance est une mesure de l'ajustement du modÃ¨le aux donnÃ©es. Elle reprÃ©sente la probabilitÃ© que les donnÃ©es observÃ©es soient gÃ©nÃ©rÃ©es par le modÃ¨le ajustÃ©. Plus la log-vraisemblance est Ã©levÃ©e, meilleure est l'ajustement du modÃ¨le aux donnÃ©es.<br>
**Deviance :**
La deviance est une mesure de l'ajustement du modÃ¨le par rapport Ã  un modÃ¨le de rÃ©fÃ©rence, souvent un modÃ¨le nul. Elle est calculÃ©e comme la diffÃ©rence entre la dÃ©viance du modÃ¨le ajustÃ© et celle du modÃ¨le de rÃ©fÃ©rence. Une deviance plus faible indique un meilleur ajustement du modÃ¨le aux donnÃ©es.<br>
**df.resid (degrÃ©s de libertÃ© rÃ©siduels) :**
Les degrÃ©s de libertÃ© rÃ©siduels reprÃ©sentent le nombre de donnÃ©es indÃ©pendantes restantes une fois que le modÃ¨le a Ã©tÃ© ajustÃ©. Ils sont utilisÃ©s pour calculer les statistiques de test et les valeurs p associÃ©es.

Les "scaled residuals" sont les rÃ©sidus du modÃ¨le. Des tests sont effectuÃ©s dessus afin de vÃ©rifier la bonne convergence et le bon ajustement du modÃ¨le.

```{r, echo=FALSE, warning=FALSE}
summary(GLMM_summary[["residuals"]])
```

Le tableau des effets alÃ©aoires est spÃ©cifique au GLMM. Il renseigne les informations sur cette partie de la formule :<br> (1 \| campagne) + (1 \| station)

```{r, echo=FALSE, warning=FALSE}
GLMM_summary[["varcor"]]
```

Enfin, il y a la partie sur les effets fixes. Cette partie permet de dresser un diagnostic sur les facteurs et la variable d'Ã©tude. Le tableau des effets fixes fournit des informations clÃ©s sur les effets estimÃ©s des variables prÃ©dictives, leur prÃ©cision et leur importance, aidant ainsi Ã  comprendre les relations entre les variables et Ã  tirer des conclusions sur les donnÃ©es.

**Estimation (Estimate):** Cette colonne indique les coefficients estimÃ©s (ou effets) de chaque variable prÃ©dictive du modÃ¨le. L'effet estimÃ© de l'"Intercept" reprÃ©sente la valeur moyenne estimÃ©e de la variable rÃ©ponse lorsque toutes les autres variables prÃ©dictives sont nulles.<br>
**Erreur standard (Std. Error):** Cette colonne indique les erreurs standard associÃ©es Ã  chaque estimation de coefficient. Les erreurs standard mesurent la variabilitÃ© de l'estimation. Des erreurs standard plus faibles indiquent des estimations plus prÃ©cises.<br>
**Valeur t (t value):** Cette colonne indique la statistique t permettant de tester l'hypothÃ¨se nulle selon laquelle le coefficient est Ã©gal Ã  zÃ©ro. Elle est calculÃ©e en divisant l'estimation par son erreur standard. Des valeurs t absolues plus Ã©levÃ©es indiquent une preuve plus forte contre l'hypothÃ¨se nulle.<br>
**Pr(>|z|):** Cette colonne indique la valeur p associÃ©e Ã  la statistique t pour chaque coefficient. Elle indique la probabilitÃ© d'observer les donnÃ©es si l'hypothÃ¨se nulle (aucun effet) Ã©tait vraie. Des valeurs p plus faibles suggÃ¨rent une preuve plus forte contre l'hypothÃ¨se nulle et indiquent que le coefficient est statistiquement significatif.<br>

```{r, echo=FALSE, warning=FALSE}
GLMM_summary[["coefficients"]]
```

La colonne "Estimate" permet de dÃ©terminer la valeur moyenne que prend la variable Ã©tudiÃ©e (dans notre exemple l'abondance totale) par modalitÃ© des variables explicatives. Dans notre exemple, l'abondance est expliquÃ©e par la variable "traitement" avec 2 modalitÃ©s (impact, Sans impact) et la variable "saison" avec 4 modalitÃ©s (winter, spring, summer, autumn). La ligne (Intercept) correspond Ã  une valeur de base. Cette valeur de base est associÃ©e Ã  une modalitÃ© de chacune de nos variables. Ainsi en hiver et avec une impact le logarithme de l'abondance (car loi Lognormale) vaut en moyenne 10.68.<br>
Pour connaitre la valeur du logarithme de l'abondance en hiver et sans impact, il faut additionner la valeur Intercept Ã  la valeur de l'estimate de la ligne "traitementSans impact" soit 10.68 + 0.12 qui vaut 10.8. Et pour vÃ©rifier si le changement est significatif, il suffit de regarder la colonne "Pr(\>\|z\|)" et vÃ©rifier si la valeur est infÃ©rieure Ã  0.05. Maintenant, pour obtenir la valeur du logarithme de l'abondance en Ã©tÃ© et avec impact, il faut additionner la valeur Intercept Ã  la valeur de l'estimate de la ligne "saisonSummer" soit 10.68 + (-0.16) soit 10.52. Enfin, en additionnant la valeur Intercept avec celle des lignes "traitementSans impact", "saisonSummer", "traitementSans impact:saisonSummer", il est possible d'obtenir la valeur du logarithme de l'abondance en Ã©tÃ© sans impact soit 10.68 + 0.12 + (-0.16) + 1.06 qui donne 11.70. Dans lâexemple, câest lâinteraction entre la saison et le traitement qui apporte des changements significativement diffÃ©rents. Ainsi, si ces deux variables Ã©taient considÃ©rÃ©es sÃ©parÃ©ment, leur effet sur lâabondance ne serait pas visible. <br> 

Et la derniÃ¨re section montre les corrÃ©lations entre les termes Ã  effet fixe du modÃ¨le. Chaque ligne et chaque colonne reprÃ©sentent un terme Ã  effet fixe, et les valeurs du tableau sont les corrÃ©lations entre ces termes. Ces corrÃ©lations sont calculÃ©es sur la base de la matrice de covariance des estimations des effets fixes. Elles indiquent comment les effets estimÃ©s des diffÃ©rents facteurs fixes du modÃ¨le sont liÃ©s les uns aux autres.

Analyse a posteriori du modÃ¨le GLMM sur l'abondance.

```{r, echo=FALSE, warning=FALSE}
plot(simulateResiduals(GLMM))
```

AprÃ¨s avoir obtenu les rÃ©sultats de la modÃ©lisation, l'application propose une sÃ©rie de graphiques pour diagnostiquer la qualitÃ© de l'ajustement du modÃ¨le, en se concentrant notamment sur l'analyse des rÃ©sidus. Ces graphiques sont gÃ©nÃ©rÃ©s Ã  l'aide du package DHARMa ("Residual Diagnostics for Hierarchical (Multi-level/Mixed) Regression Models") dans R.

Le graphique de gauche, appelÃ© "QQ plot residual", est une reprÃ©sentation des rÃ©sidus attendus par rapport aux observations rÃ©elles. Dans ce graphique, chaque point reprÃ©sente un rÃ©sidu calculÃ© par le modÃ¨le pour une observation donnÃ©e. IdÃ©alement, ces points devraient suivre de prÃ¨s une ligne rouge diagonale, ce qui signifierait que les rÃ©sidus sont distribuÃ©s de maniÃ¨re approximativement normale. Si les points s'Ã©loignent de maniÃ¨re significative de cette ligne rouge, cela suggÃ¨re une mauvaise adÃ©quation du modÃ¨le aux donnÃ©es observÃ©es.

En plus de la visualisation des rÃ©sidus, l'outil DHARMa propose trois tests pour Ã©valuer la qualitÃ© de l'ajustement du modÃ¨le :<br>
**Test de Kolmogorov-Smirnov :**<br>
Ce test d'hypothÃ¨se est utilisÃ© pour Ã©valuer si l'Ã©chantillon de rÃ©sidus suit une loi de distribution connue, dÃ©terminÃ©e par sa fonction de rÃ©partition continue. Une dÃ©viation significative des rÃ©sidus par rapport Ã  cette distribution attendue peut indiquer une inadÃ©quation du modÃ¨le aux donnÃ©es.<br>
**Un test de Dispersion :**<br>
Ce test compare l'Ã©cart-type observÃ© des rÃ©sidus Ã  celui qui serait attendu en se basant sur la simulation des donnÃ©es. Si la diffÃ©rence est significative, cela peut suggÃ©rer une sous- ou sur-dispersion des rÃ©sidus par rapport aux attentes du modÃ¨le. <br>
**Un test de Valeur Aberrante :** <br>
Ce test vise Ã  vÃ©rifier si le nombre d'observations dont les rÃ©sidus se trouvent en dehors de l'enveloppe de simulation est conforme aux attentes du modÃ¨le. Une dÃ©viation significative de ce nombre peut indiquer la prÃ©sence de valeurs aberrantes ou une mauvaise adÃ©quation du modÃ¨le.
    
Chaque test fournit une mesure de la dÃ©viation par rapport aux attentes du modÃ¨le avec une p-value associÃ©e. Une p-value faible (< 0,05) indique gÃ©nÃ©ralement une dÃ©viation significative par rapport aux attentes du modÃ¨le, tandis qu'une p-value Ã©levÃ©e suggÃ¨re que la dÃ©viation observÃ©e pourrait Ãªtre due au hasard et n'est pas statistiquement significative. Si cette dÃ©viation est significative, elle est signalÃ©e en rouge, indiquant que le test correspondant n'est pas conforme aux attentes du modÃ¨le. Ces diagnostics aident Ã  identifier les inadÃ©quations entre le modÃ¨le et les donnÃ©es observÃ©es et Ã  guider les ajustements nÃ©cessaires pour obtenir un modÃ¨le plus appropriÃ©.

Sur le graphique de droite, des tests sont rÃ©alisÃ©s sur l'uniformitÃ© et l'homogÃ©nÃ©itÃ© de la variance des groupes Ã©valuÃ©s dans le modÃ¨le. Le test "within-group deviation from uniformity" est un boxplot qui reprÃ©sente la distribution des dÃ©viations des rÃ©sidus au sein de chaque groupe dÃ©fini par les modalitÃ©s des facteurs qualitatifs de votre modÃ¨le. Chaque groupe est reprÃ©sentÃ© par une boÃ®te, oÃ¹ la mÃ©diane est indiquÃ©e par une ligne Ã  l'intÃ©rieur de la boÃ®te, le premier et le troisiÃ¨me quartile sont reprÃ©sentÃ©s par les bords infÃ©rieur et supÃ©rieur de la boÃ®te, et les moustaches s'Ã©tendent jusqu'aux valeurs maximale et minimale. Les points au-delÃ  de cette limite sont considÃ©rÃ©s comme des valeurs aberrantes. L'objectif de ce test est d'identifier les groupes pour lesquels les rÃ©sidus prÃ©sentent des variations importantes par rapport Ã  une distribution uniforme (ceux-ci apparaissent alors en rouge). Des variations importantes peuvent indiquer une inadÃ©quation du modÃ¨le pour certains groupes spÃ©cifiques. <br> Le deuxiÃ¨me test correspond Ã  un test de Levene. Le test de Levene est utilisÃ© pour Ã©valuer si les variances des rÃ©sidus diffÃ¨rent significativement entre les groupes dÃ©finis par les modalitÃ©s des facteurs qualitatifs. Il teste l'hypothÃ¨se nulle selon laquelle les variances sont Ã©gales entre tous les groupes. Une p-valeur faible (gÃ©nÃ©ralement < 0,05) indique une diffÃ©rence significative dans les variances des rÃ©sidus entre les groupes, suggÃ©rant que l'hypothÃ¨se d'homogÃ©nÃ©itÃ© des variances n'est pas valide.

Dans le cas particulier oÃ¹ il y a trop de modalitÃ©s diffÃ©rentes dÃ» Ã  de multiple covariables (ou que les variables explicatives soient quantitatives), la partie sur l'uniformitÃ© et l'homogÃ©nÃ©itÃ© des groupes est remplacÃ© par une reprÃ©sentation des rÃ©sidus du modÃ¨le en comparaison avec les prÃ©dictions du modÃ¨le. Si n'y a aucun problÃ¨me alors la phrase : "No significant problems detected" s'affiche en haut du graphique. Si des dÃ©viations des rÃ©sidus par rapport Ã  une distribution uniforme Ã  travers diffÃ©rentes quantiles sont significatives ou que les dÃ©viations quantiles observÃ©es sont statistiquement significatives, les tests asociÃ©s apparaÃ®ssent en rouge, ce qui suggÃ¨re une inadÃ©quation du modÃ¨le pour certains aspects des donnÃ©es. Enfin, les valeurs aberrantes de la simulation (points de donnÃ©es qui se situent en dehors de la plage des valeurs simulÃ©es) sont mises en Ã©vidence par des Ã©toiles rouges. Ces points doivent Ãªtre interprÃ©tÃ©s avec prÃ©caution, car nous ne savons pas "Ã  quel point" ces valeurs s'Ã©cartent des attentes du modÃ¨le. L'important est de vÃ©rifier que les tests de vÃ©rification ne soient pas significatifs.

<br>Dans le cas de l'exemple, les tests de Kolmogorov-Smirnov, de Valeur aberrante et de dispersion ne sont pas significatifs donc il n'y a pas de problÃ¨me. Si un de ces tests s'affichaient en rouge, cela indiquerait que le modÃ¨le n'est pas optimal et il serait possible alors de chercher un autre modÃ¨le qui s'ajusterait mieux. Comme ces modÃ¨les se basent sur des donnÃ©es rÃ©elles, il est parfois impossible de trouver un modÃ¨le parfait. Il faut alors choisir le modÃ¨le avec le moins d'avertissement. On peut voir Ã©galement que le test d'uniformitÃ© est validÃ© mais pas celui d'homogÃ©nÃ©itÃ©. Une fois le modÃ¨le validÃ©, vous pouvez changer d'onglet et passer Ã  la visualisation des effets associÃ©s au modÃ¨le.<br> Si l'on rajoute la covariable annÃ©e Ã  notre modÃ¨le GLMM, cela reprÃ©sente 96 modalitÃ©s. Le graphique comparant les groupes est remplacÃ© par un graphique comparant les prÃ©dictions globales avec les rÃ©sidus du modÃ¨le.<br>

```{r, echo=FALSE, warning=FALSE}
long_glmm <- glmer(log(Abun) ~ traitement * saison + year + (1|campagne) + (1|station), data = dataset, family = gaussian(link = identity))
plot(simulateResiduals(long_glmm))
```

<br>Dans certains cas, la modÃ©lisation GLMM ne converge pas. Cela signifie que les donnÃ©es disponibles ne permettent pas Ã  l'algorithme de calcul, associÃ© Ã  la formulation du modÃ¨le dÃ©cidÃ©e par l'utilisateur, d'estimer des valeurs de paramÃ¨tres. Dans ces cas lÃ , un message d'erreur apparait : *"Il y a une erreur lors de la modÃ©lisation. Veuillez changer la loi ou le modÃ¨le."* Il est possible aussi que le modÃ¨le produise des rÃ©sultats mais dont l'analyse des rÃ©sidus a posteriori n'est pas satisfaisante. <br>Exemple :

```{r, echo=FALSE, warning=FALSE}
bad_glmm <- glmer(Abun ~ traitement * saison + (1|campagne) + (1|station), data = dataset, family = gaussian(link = identity))
plot(simulateResiduals(bad_glmm))
```

<br> Lorsque la modÃ©lisation par GLMM ne converge pas, il faut lui prÃ©fÃ©rer des mÃ©thodes de modÃ©lisation associÃ©es Ã   des algorithmes de calcul plus sipmles, c'est-Ã -dire avec moins de paramÃ¨tres Ã  estimer. L'outil GranulatShiny en propose deux : le GLM et la PERMANOVA.<br> NOTA BENE : lorsque le jeu de donnÃ©es utilisÃ© pour la modÃ©lisation est constituÃ© de 30 observations non nulles ou moins, il est prÃ©fÃ©rable de s'en tenir Ã  la mÃ©thode la moins coÃ»teuse en termes de calcul, Ã  savoir la PERMANOVA.

##### Â Â Â Â [Generalized linear model]{.underline}

Reproduction de la sortie R du modÃ¨le GLM sur l'abondance

```{r, echo=FALSE, warning=FALSE}

GLM <- glm(log(Abun) ~ traitement*saison, data = dataset, family = gaussian(link = identity))
summary(GLM)
```

Le GLM sort des rÃ©sultats proches de celui du GLMM et s'analyse de la mÃªme maniÃ¨re. Cependant, celui est moins prÃ©cis. Il ne prend pas en compte les effets alÃ©atoires induits par l'environnement ou la mÃ©thode utilisÃ©e.

##### Â Â Â Â [Analysis of variance by permutation]{.underline}

MÃ©thode de PERMANOVA intÃ©grÃ©e dans l'application

Dans le cas de Granulatshiny, la PERMANOVA est appliquÃ©e sur une matrice colonne regroupant des la plupart du temps des abondances ou des biomasses. Ainsi les doubles zÃ©ros ne peuvent donc pas Ãªtre pris en compte dans le calcul de ressemblance. Donc les donnÃ©es sont quantitatives et les doubles zÃ©ros ne sont pas considÃ©rÃ© donc la mÃ©thode qui semble la plus appropriÃ© est celle du coefficient de Bray-Curtis. Câest le choix qui avait Ã©tÃ© fait Ã  lâorigine. <br> Lâindice de dissimilaritÃ© de Bray-Curtis, est utilisÃ© en Ã©cologie et biologie pour Ã©valuer la dissimilaritÃ© entre deux Ã©chantillons donnÃ©s, en termes d'abondance de taxons prÃ©sents dans chacun de ces Ã©chantillons. Elle est compris entre 0 (les deux Ã©chantillons ont la mÃªme composition) et 1 (les Ã©chantillons sont totalement dissemblables). La dissimilaritÃ© de Bray-Curtis est souvent utilisÃ©e dans la littÃ©rature. Elle est asymÃ©trique et semimÃ©trique. Elle se calcule comme ceci : $$ d_{jk}=\frac{\sum_{i} |x_{ij}-x_{ik}|}{\sum_{i} (x_{ij}+x_{ik})} $$ oÃ¹ *i = colonne; j,k = lignes comparÃ©es; x = valeurs d'abondances*

Dans le cas oÃ¹ la matrice dâentrÃ©e nâa quâune seule colonne soit une seule espÃ¨ce et que dans les valeurs dâentrÃ©es, il existe des zÃ©ros, il arrive parfois que le dÃ©nominateur soit Ã©gale Ã  zÃ©ro ce qui nâest pas possible et donc crÃ©er une erreur dans la matrice de distance. Câest le cas pour tous les indicateurs habituellement utilisÃ©s pour des donnÃ©es d'abondances qui pondÃ¨rent leur distance en fonction de lâabondance totale dans les sites comparÃ©s. Cette mÃ©thode ne pouvant s'utiliser dans notre cas, un autre coefficient de calcul de distance a Ã©tÃ© recherchÃ©. Celui-ci ne devait Ã©galement pas tenir compte des doubles zÃ©ros sur des quantitatives.

La mÃ©thode retenue est la mÃ©trique du chiÂ². Celle-ci donne davantage de poids aux espÃ¨ces rares quâaux espÃ¨ces communes. Son utilisation est recommandÃ©e lorsque les espÃ¨ces rares sont de bons indicateurs de conditions Ã©cologiques particuliÃ¨res. Pour appliquer cette mÃ©thode, il faut d'abord standardiser les donnÃ©es selon la mÃ©thode du chiÂ² comme ceci : $$ x'_{ij}=\frac{x_{ij}}{\sum x_{j} * \sqrt \sum x_{i}} $$ oÃ¹ *i = colonne; j = ligne; x = valeurs d'abondances*

Ensuite, on calcule la matrice de distance en calculant la distance euclidienne sur la matrice de donnÃ©es standardisÃ©es. $$d_{jk}=\sqrt \sum_{i} (x_{ij}-x_{ik})Â² $$ oÃ¹ *i = colonne; j,k = lignes comparÃ©es; x = valeurs d'abondances standardisÃ©es*

L'inconvÃ©nient avec cette mÃ©thode, le calcul des effets se produisant sur des donnÃ©es transformÃ©es, il est n'est pas possible de quantifier directement l'impact d'un effet sur la variable initiale. On ne peut donc pas dire si un effet est plus ou moins fort sur la donnÃ©e initiale car celui-ci s'applique Ã  la donnÃ©e transformÃ©e. Par contre, si un effet est considÃ©rÃ© significatif sur les donnÃ©es transformÃ©es alors il l'est Ã©galement sur les donnÃ©es initiales.

Reproduction de la sortie R de la PERMANOVA sur l'abondance

```{r, echo=FALSE, warning=FALSE}

vector <- decostand(dataset["Abun"],"chi.square", MARGIN = 2)
vector <- data.frame(as.numeric(vector[1,]))
names(vector) <- "Abun"
dist <- vegdist(vector, method = "euclidean")
result <- adonis2(dist~traitement*saison, data = dataset, permutations = 999)
result

```

Dans le tableu de sortie de la PERMANOVA, on retrouve le nombre de permutations et la formule du modÃ¨le. Ensuite on retouve plusieurs indicateurs associÃ©s Ã  chaque covariable explicative.<br> **Df (degrÃ©s de libertÃ©)**: Cette colonne indique les degrÃ©s de libertÃ© associÃ©s Ã  chaque terme du modÃ¨le.<br> **SumOfSqs (Somme des carrÃ©s)**: Cette colonne indique la somme des distances au carrÃ© entre les observations dans l'espace multivariÃ©.<br> **R2 (R-carrÃ©)**: Cette colonne indique la proportion de variance expliquÃ©e par chaque terme du modÃ¨le. Par exemple, pour "traitement", 8,34 % de la variation des donnÃ©es peut Ãªtre expliquÃ©e par le facteur traitement.<br> **F (statistique F)** : Cette colonne indique la statistique F pour chaque terme, qui vÃ©rifie si la variation expliquÃ©e par ce terme est significativement plus importante que ce que l'on attendrait du hasard. Des valeurs F plus Ã©levÃ©es indiquent des preuves plus solides contre l'hypothÃ¨se nulle d'absence d'effet.<br> **Pr(\>F) (valeur p)**: Cette colonne indique la valeur p associÃ©e Ã  la statistique F pour chaque terme. Elle indique la probabilitÃ© d'observer les donnÃ©es si l'hypothÃ¨se nulle d'absence d'effet (c'est-Ã -dire si toutes les moyennes des groupes sont Ã©gales) Ã©tait vraie. Des valeurs p plus faibles suggÃ¨rent une preuve plus forte contre l'hypothÃ¨se nulle et indiquent que le terme est un prÃ©dicteur significatif de la variation.<br> La PERMANOVA est la mÃ©thode Ã  utiliser en derniÃ¨re. Elle esty moins prÃ©cise et apporte moins d'informations qu'un GLMM ou qu'un GLM.

Analyse complÃ©mentaire a posteriori de la PERMANOVA sur l'abondance.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

p_value <- result["traitement","Pr(>F)"]

legend <- annotate("text", x = 0.65,
                   y = max(log(as.numeric(data)+1)),
                   label = paste("* p = ",p_value, sep = ""),
                   colour = "red", size = 5)

plot <- ggplot(dataset, aes(x = traitement, y = log(Abun+1)))+
  geom_boxplot()+
  labs(title = paste("Boxplot of ", "log(Abun)"," by impact", sep=""),
                x = "", y = "")+ legend
plot
```

<br>Les mÃªmes boites Ã  moustaches que dans l'onglet ReprÃ©sentation des donnÃ©es sont affichÃ©es. Sauf que si la variable de comparaison (dans l'exemple c'est le traitement) a un effet significatif sur la variable expliquÃ©e (ici l'abondance) alors la p-value apparait en rouge en haut Ã  gauche du graphique. S'il n'y a pas d'effet dÃ©tectÃ© pendant la PERMANOVA alors le message "Pas d'effet" apparait en haut Ã  gauche du graphique.

### Effects representation

Cet onglet permet de visualiser graphiquement les effets des variables explicatives sur la variable expliquÃ©e dans le cas d'un GLMM ou d'un GLM. Dans le cas d'une PERMANOVA, cette section n'est pas sollicitÃ©e et la fenÃªtre graphique sera blanche. Cette partie retransforme les estimates du modÃ¨le en l'unitÃ© initiale (dans le cas de l'abondance c'est un nombre par kmÂ²). Ainsi on peut voir la valeur moyenne de l'abondance en fonction de la saison et du traitement. D'abord vous devez choisir les deux prÃ©dicteurs Ã  reprÃ©senter.<br>

```{r, echo=FALSE, warning=FALSE}
      #choix du terme
      box(
        solidHeader = F,
        status = "danger",
        selectInput( "pred_1",
          label = "Choississez un premier prÃ©dicteur",
          choices = c("traitement","saison"),
          multiple = F,
          selected = c("traitement")),
        selectInput("pred_2",
          label = "Choississez un second prÃ©dicteur",
          choices = c("traitement","saison"),
          multiple = F,
          selected = c("saison")),
        width = "100%"
        )
```

Si vous avez plusieurs covariables vous devez les fixer afin de pouvoir visualiser le graphique. <br> Dans l'exemple d'un GLM qui regarde l'abundance totale en fonction du traitement et de la saison, voici le graphique obtenu :

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggpredict(GLM, terms = c("traitement", "saison"))|> plot()
```

<br>Ce graphique est une autre maniÃ¨re de reprÃ©senter le tableau de sortie du modÃ¨le de l'onglet prÃ©cÃ©dent.

### Statistical power

Cette partie est en cours de dÃ©veloppement. L'outil antÃ©rieur construit par Mathis Cambreling fonctionne seulement pour le jeu de donnÃ©es ayant servi de base Ã  ses calculs. L'outil n'Ã©tant pas gÃ©nÃ©ralisable, celui-ci a Ã©tÃ© retirÃ© pour assurer la stabilitÃ© actuelle de l'application. Un autre outil est en cours de dÃ©veloppement.<br>

## Bibliography

**Anderson MJ** (2017) Permutational Multivariate Analysis of Variance (PERMANOVA). Wiley StatsRef: Statistics Reference Online. John Wiley & Sons, Ltd, pp 1â15

**Avezard C, Lavarde P, Pichon A, Legait B, Wallard I** (2017) Impact environnemental et Ìeconomique des activit Ìes dâexploration ou dâexploitation des ressources minÃ©rales marines.

**Bolker BM** (2008) Ecological Models and Data in R. doi: 10.2307/j.ctvcm4g37

**Bolker BM, Brooks ME, Clark CJ, Geange SW, Poulsen JR, Stevens MHH, White J-SS** (2009) Generalized linear mixed models: a practical guide for ecology and evolution. Trends in Ecology & Evolution 24: 127â135

**Colwell R **(2009) Biodiversity: concepts, patterns, and measurement. The Princeton Guide to Ecology. pp 257â263

**David V** (2019) Statistique pour les sciences environnementales. ISTE Editions, Londres, Royaume-Uni

**Gorodetska N, Behaghel G, Dalifard T, Daniel F, Grison X, Hausermann B, Laurent C, De Lantivy S, Lefebvre E, Panonacle H, et al** (2023) Lâ Ìeconomie bleue en France.

**Gregorius H-R, Gillet EM **(2008) Generalized Simpson-diversity. Ecological Modelling 211: 90â96

**Legendre P, Gallagher ED** (2001) Ecologically meaningful transformations for ordination of species data. Oecologia 129: 271â280

**Methratta ET** (2020) Monitoring fisheries resources at offshore wind farms: BACI vs. BAG designs. ICES Journal of Marine Science 77: 890â900

**MinistÃ¨re de lâEnvironnement de lâÌenergie et de la mer** (2016) Guide mÃ©thodologique pour l'Ã©laboration des documents dâorientations pour une gestion durable des granulats marins (DOGGM). MinistÃ¨re de lâEnvironnement, de lâEnergie et de la Mer. Paris

**MTE, UNPG, IFREMER, DREAL, DIRM** (2023) Guide technique pour lâÃ©laboration des Ìetudes dâimpact prÃ©alables Ã  la recherche et lâexploitation des granulats marins. 48

**Oksanen J** (2022) Dissimilarity Indices for Community Ecologists.

**Ortiz-Burgos S **(2016) Shannon-Weaver Diversity Index. In MJ Kennish, ed, Encyclopedia of Estuaries. Springer Netherlands, Dordrecht, pp 572â573

**Parent S-E** (2020) Analyse et modÃ©lisation dâagroÃ©cosystÃ¨mes.

**Rassweiler A, Okamoto DK, Reed DC, Kushner DJ, Schroeder DM, Lafferty KD** (2021) Improving the ability of a BACI design to detect impacts within a kelp-forest community. Ecological Applications 31: e02304

**Seger KD, Sousa-Lima R, Schmitter-Soto JJ, Urban ER** (2021) Editorial: Before-After Control-Impact (BACI) Studies in the Ocean. Frontiers in Marine Science 8:

**Shannon CE** (1948) A mathematical theory of communication. The Bell System Technical Journal 27: 379â423

**Smokorowski KE, Randall RG** (2017) Cautions on using the Before-After-Control-Impact design in environmental effects monitoring programs. FACETS 2: 212â232

**Underwood AJ** (1994) On Beyond BACI: Sampling Designs that Might Reliably Detect Environmental Disturbances. Ecological Applications 4: 4â15

**Walker R, Bokuniewicz H, Carlin D, Cato I, Dijkshoorn C, Backer AD, Dalfsen J van, Desprez M, Howe L, Robertsdottir BG, et al** (2016) Effects of extraction of marine sediments on the marine environment 2005-2011. doi: 10.17895/ices.pub.5498

**WGEXT** (2019) Working Group on the Effects of Extraction of Marine Sediments on the Marine Ecosystem (WGEXT). doi: 10.17895/ices.pub.5
